{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unigram CLASSIFIER\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "stop_words = {\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n",
    "    'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing',\n",
    "    \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\",\n",
    "    'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself',\n",
    "    'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is',\n",
    "    \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no',\n",
    "    'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so',\n",
    "    'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "    'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those',\n",
    "    'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\",\n",
    "    \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while',\n",
    "    'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\",\n",
    "    \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'\n",
    "}\n",
    "\n",
    "def U_preprocess(text):\n",
    "    # Lowercase and keep only words\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    bigrams = []\n",
    "    words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "    # for i in range(1, len(words)):\n",
    "    #     bigrams.append(f\"{words[i-1]}_{words[i]}\")\n",
    "\n",
    "    # trigrams = []\n",
    "    # for i in range(2, len(words)):\n",
    "    #     trigrams.append(f\"{words[i-2]}_{words[i-1]}_{words[i]}\")\n",
    "\n",
    "    return words\n",
    "\n",
    "def UnigramClassifier(alpha, X_train, X_val, y_train):\n",
    "    projs = [\"A\", \"S\", \"G\", \"W\"]\n",
    "    word_counts = defaultdict(Counter)\n",
    "    total_words = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    all_words = Counter()\n",
    "    num_projs = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    total_instances = len(X_train)\n",
    "\n",
    "    word_counts[\"A\"] = Counter()\n",
    "    word_counts[\"S\"] = Counter()\n",
    "    word_counts[\"G\"] = Counter()\n",
    "    word_counts[\"W\"] = Counter()\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        pred, desc = y_train[i], X_train[i]\n",
    "\n",
    "        pp_desc = U_preprocess(desc)\n",
    "\n",
    "        word_counts[pred].update(pp_desc)\n",
    "        total_words[pred] += len(pp_desc)\n",
    "        all_words.update(pp_desc)\n",
    "        num_projs[pred] += 1\n",
    "\n",
    "    vocab_size = len(all_words)\n",
    "\n",
    "    class_weights = {proj: 1.0 / num_projs[proj] for proj in projs}\n",
    "    total_weight = sum(class_weights.values()) \n",
    "    proj_probs = []\n",
    "    for proj in projs:\n",
    "        weighted_prob = class_weights[proj] / total_weight  # Normalize so that sum is 1\n",
    "        proj_probs.append(weighted_prob)\n",
    "\n",
    "    word_probs = defaultdict(Counter)\n",
    "    word_probs[\"A\"] = Counter()\n",
    "    word_probs[\"S\"] = Counter()\n",
    "    word_probs[\"G\"] = Counter()\n",
    "    word_probs[\"W\"] = Counter()\n",
    "\n",
    "    #Calculating probabilities for each word/bigram\n",
    "    for proj in projs:\n",
    "        focus_dict = word_counts[proj]\n",
    "        denom = total_words[proj] + alpha*total_instances\n",
    "        \n",
    "        #Calculating likelihoods with Laplace smoothing\n",
    "        for word in all_words:\n",
    "            if word not in focus_dict:\n",
    "                word_probs[proj][word] = alpha/denom\n",
    "            else:\n",
    "                word_probs[proj][word] = (focus_dict[word]+alpha)/denom\n",
    "        \n",
    "    #Initialzing classification variables\n",
    "    temp = np.log(proj_probs)\n",
    "    class_probs = {}\n",
    "    for i in range(len(temp)):\n",
    "        class_probs[projs[i]] = temp[i]\n",
    "    classifications = []\n",
    "\n",
    "    #Classify test data\n",
    "    for desc in X_val:\n",
    "        pp_desc = U_preprocess(desc)\n",
    "\n",
    "        cur_class_probs = class_probs.copy()\n",
    "        for proj in projs:\n",
    "            #Unigram Probs\n",
    "            for word in pp_desc:\n",
    "                cur_class_probs[proj] += np.log(word_probs[proj].get(word, alpha / denom))\n",
    "            \n",
    "            # #Bigram Probs\n",
    "            # for i in range(1, len(pp_desc)):\n",
    "            #     cur_bigram = f\"{pp_desc[i-1]}_{pp_desc[i]}\"\n",
    "            #     cur_class_probs[proj] += np.log(word_probs[proj].get(cur_bigram, alpha / denom))\n",
    "\n",
    "            # #Trigram Probs\n",
    "            # for i in range(2, len(pp_desc)):\n",
    "            #     cur_trigram = f\"{pp_desc[i-2]}_{pp_desc[i-1]}_{pp_desc[i]}\"\n",
    "            #     cur_class_probs[proj] += np.log(word_probs[proj].get(cur_trigram, alpha / denom))\n",
    "\n",
    "        #find project with highest probability, and make that the prediction\n",
    "        best_proj = max(cur_class_probs, key=cur_class_probs.get)\n",
    "        classifications.append(best_proj)\n",
    "    \n",
    "    return classifications\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigram CLASSIFIER\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "stop_words = {\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n",
    "    'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing',\n",
    "    \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\",\n",
    "    'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself',\n",
    "    'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is',\n",
    "    \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no',\n",
    "    'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so',\n",
    "    'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "    'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those',\n",
    "    'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\",\n",
    "    \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while',\n",
    "    'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\",\n",
    "    \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'\n",
    "}\n",
    "\n",
    "def B_preprocess(text):\n",
    "    # Lowercase and keep only words\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    bigrams = []\n",
    "    for i in range(1, len(words)):\n",
    "        bigrams.append(f\"{words[i-1]}_{words[i]}\")\n",
    "    return bigrams\n",
    "\n",
    "def BigramClassifier(alpha, X_train, X_val, y_train):\n",
    "    projs = [\"A\", \"S\", \"G\", \"W\"]\n",
    "    word_counts = defaultdict(Counter)\n",
    "    total_words = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    all_words = Counter()\n",
    "    num_projs = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    total_instances = len(X_train)\n",
    "\n",
    "    word_counts[\"A\"] = Counter()\n",
    "    word_counts[\"S\"] = Counter()\n",
    "    word_counts[\"G\"] = Counter()\n",
    "    word_counts[\"W\"] = Counter()\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        pred, desc = y_train[i], X_train[i]\n",
    "\n",
    "        pp_desc = B_preprocess(desc)\n",
    "\n",
    "        word_counts[pred].update(pp_desc)\n",
    "        total_words[pred] += len(pp_desc)\n",
    "        all_words.update(pp_desc)\n",
    "        num_projs[pred] += 1\n",
    "\n",
    "    vocab_size = len(all_words)\n",
    "\n",
    "    class_weights = {proj: 1.0 / num_projs[proj] for proj in projs}\n",
    "    total_weight = sum(class_weights.values()) \n",
    "    proj_probs = []\n",
    "    for proj in projs:\n",
    "        weighted_prob = class_weights[proj] / total_weight  # Normalize so that sum is 1\n",
    "        proj_probs.append(weighted_prob)\n",
    "\n",
    "    word_probs = defaultdict(Counter)\n",
    "    word_probs[\"A\"] = Counter()\n",
    "    word_probs[\"S\"] = Counter()\n",
    "    word_probs[\"G\"] = Counter()\n",
    "    word_probs[\"W\"] = Counter()\n",
    "\n",
    "    #Calculating probabilities for each word/bigram\n",
    "    for proj in projs:\n",
    "        focus_dict = word_counts[proj]\n",
    "        denom = total_words[proj] + alpha*total_instances\n",
    "        \n",
    "        #Calculating likelihoods with Laplace smoothing\n",
    "        for word in all_words:\n",
    "            if word not in focus_dict:\n",
    "                word_probs[proj][word] = alpha/denom\n",
    "            else:\n",
    "                word_probs[proj][word] = (focus_dict[word]+alpha)/denom\n",
    "        \n",
    "    #Initialzing classification variables\n",
    "    temp = np.log(proj_probs)\n",
    "    class_probs = {}\n",
    "    for i in range(len(temp)):\n",
    "        class_probs[projs[i]] = temp[i]\n",
    "    classifications = []\n",
    "\n",
    "    #Classify test data\n",
    "    for desc in X_val:\n",
    "        pp_desc = B_preprocess(desc)\n",
    "\n",
    "        cur_class_probs = class_probs.copy()\n",
    "        for proj in projs:\n",
    "            # for i in range(1, len(pp_desc)):\n",
    "            #     cur_bigram = f\"{pp_desc[i-1]}_{pp_desc[i]}\"\n",
    "            #     cur_class_probs[proj] += np.log(word_probs[proj].get(cur_bigram, alpha / denom))\n",
    "            for bigram in pp_desc:\n",
    "                cur_class_probs[proj] += np.log(word_probs[proj].get(bigram, alpha / denom))\n",
    "\n",
    "        #find project with highest probability, and make that the prediction\n",
    "        best_proj = max(cur_class_probs, key=cur_class_probs.get)\n",
    "        classifications.append(best_proj)\n",
    "    \n",
    "    return classifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UniBigram CLASSIFIER\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "stop_words = {\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n",
    "    'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing',\n",
    "    \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\",\n",
    "    'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself',\n",
    "    'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is',\n",
    "    \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no',\n",
    "    'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so',\n",
    "    'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "    'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those',\n",
    "    'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\",\n",
    "    \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while',\n",
    "    'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\",\n",
    "    \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'\n",
    "}\n",
    "\n",
    "def T_preprocess(text):\n",
    "    # Lowercase and keep only words\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    # words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    bigrams = []\n",
    "    for i in range(2, len(words)):\n",
    "        bigrams.append(f\"{words[i-2]}_{words[i-1]}_{words[i]}\")\n",
    "\n",
    "    return bigrams\n",
    "\n",
    "def UniBigramClassifier(alpha, X_train, X_val, y_train):\n",
    "    projs = [\"A\", \"S\", \"G\", \"W\"]\n",
    "    word_counts = defaultdict(Counter)\n",
    "    total_words = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    all_words = Counter()\n",
    "    num_projs = {\"A\":0,\"S\":0,\"G\":0,\"W\":0}\n",
    "    total_instances = len(X_train)\n",
    "\n",
    "    word_counts[\"A\"] = Counter()\n",
    "    word_counts[\"S\"] = Counter()\n",
    "    word_counts[\"G\"] = Counter()\n",
    "    word_counts[\"W\"] = Counter()\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        pred, desc = y_train[i], X_train[i]\n",
    "\n",
    "        pp_desc = T_preprocess(desc)\n",
    "\n",
    "        word_counts[pred].update(pp_desc)\n",
    "        total_words[pred] += len(pp_desc)\n",
    "        all_words.update(pp_desc)\n",
    "        num_projs[pred] += 1\n",
    "\n",
    "    vocab_size = len(all_words)\n",
    "\n",
    "    class_weights = {proj: 1.0 / num_projs[proj] for proj in projs}\n",
    "    total_weight = sum(class_weights.values()) \n",
    "    proj_probs = []\n",
    "    for proj in projs:\n",
    "        weighted_prob = class_weights[proj] / total_weight  # Normalize so that sum is 1\n",
    "        proj_probs.append(weighted_prob)\n",
    "\n",
    "    word_probs = defaultdict(Counter)\n",
    "    word_probs[\"A\"] = Counter()\n",
    "    word_probs[\"S\"] = Counter()\n",
    "    word_probs[\"G\"] = Counter()\n",
    "    word_probs[\"W\"] = Counter()\n",
    "\n",
    "    #Calculating probabilities for each word/bigram\n",
    "    for proj in projs:\n",
    "        focus_dict = word_counts[proj]\n",
    "        denom = total_words[proj] + alpha*total_instances\n",
    "        \n",
    "        #Calculating likelihoods with Laplace smoothing\n",
    "        for word in all_words:\n",
    "            if word not in focus_dict:\n",
    "                word_probs[proj][word] = alpha/denom\n",
    "            else:\n",
    "                word_probs[proj][word] = (focus_dict[word]+alpha)/denom\n",
    "        \n",
    "    #Initialzing classification variables\n",
    "    temp = np.log(proj_probs)\n",
    "    class_probs = {}\n",
    "    for i in range(len(temp)):\n",
    "        class_probs[projs[i]] = temp[i]\n",
    "    classifications = []\n",
    "\n",
    "    #Classify test data\n",
    "    for desc in X_val:\n",
    "        pp_desc = T_preprocess(desc)\n",
    "\n",
    "        cur_class_probs = class_probs.copy()\n",
    "        for proj in projs:\n",
    "            #Unigram Probs\n",
    "            for word in pp_desc:\n",
    "                cur_class_probs[proj] += np.log(word_probs[proj].get(word, alpha / denom))\n",
    "            \n",
    "            # #Bigram Probs\n",
    "            # for i in range(1, len(pp_desc)):\n",
    "            #     cur_bigram = f\"{pp_desc[i-1]}_{pp_desc[i]}\"\n",
    "            #     cur_class_probs[proj] += np.log(word_probs[proj].get(cur_bigram, alpha / denom))\n",
    "\n",
    "        #find project with highest probability, and make that the prediction\n",
    "        best_proj = max(cur_class_probs, key=cur_class_probs.get)\n",
    "        classifications.append(best_proj)\n",
    "    \n",
    "    return classifications\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EnsembleClassifier(alpha1, alpha2, alpha3, X_train, X_val, y_train):\n",
    "    #Get each classifier's predictions\n",
    "    c1 = UnigramClassifier(alpha1, X_train, X_val, y_train)\n",
    "    c2 = BigramClassifier(alpha2, X_train, X_val, y_train)\n",
    "    c3 = UniBigramClassifier(alpha3, X_train, X_val, y_train)\n",
    "    \n",
    "    classifications = []\n",
    "    #Compare the predictions --> choose majority\n",
    "    for i in range(len(c1)):\n",
    "       \n",
    "        comp = Counter([c1[i], c2[i], c3[i]])\n",
    "\n",
    "        #if they all have different guesses: use c1 guess\n",
    "        if len(comp) == 3:\n",
    "            classifications.append(c2[i])\n",
    "        else: #if not, guess the majority\n",
    "            classifications.append(max(comp, key=comp.get))\n",
    "\n",
    "    return classifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Fold Cross-Validation Test on Classifier(s):\n",
      "\n",
      "Fold 1 Accuracy: 98.41%.\n",
      "Fold 2 Accuracy: 97.39%.\n",
      "Fold 3 Accuracy: 97.39%.\n",
      "Fold 4 Accuracy: 98.18%.\n",
      "Fold 5 Accuracy: 98.41%.\n",
      "\n",
      "Average cross-validated accuracy is 97.95%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#5 Fold Cross-Validation Method\n",
    "def CheckAccuracy(classifications, y_val):\n",
    "    correct = 0\n",
    "    for i in range(len(classifications)):\n",
    "        if classifications[i] == y_val[i]:\n",
    "            correct += 1\n",
    "    return correct/len(classifications)\n",
    "    \n",
    "\n",
    "def CrossValidate():\n",
    "    print(f\"5 Fold Cross-Validation Test on Classifier(s):\\n\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    X = df[\"Description\"].values\n",
    "    y = df[\"Class\"].values\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        # Split the data\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        # classifications = UnigramClassifier(6.5, X_train, X_val, y_train)\n",
    "        # accuracy = CheckAccuracy(classifications, y_val)\n",
    "        # accuracies.append(accuracy)\n",
    "\n",
    "        # classifications = BigramClassifier(0.007, X_train, X_val, y_train)\n",
    "        # accuracy = CheckAccuracy(classifications, y_val)\n",
    "        # accuracies.append(accuracy)\n",
    "\n",
    "        # classifications = UniBigramClassifier(1, X_train, X_val, y_train)\n",
    "        # accuracy = CheckAccuracy(classifications, y_val)\n",
    "        # accuracies.append(accuracy)\n",
    "\n",
    "        classifications = EnsembleClassifier(6.5, 0.007, 2, X_train, X_val, y_train)\n",
    "        # print(classifications)\n",
    "        accuracy = CheckAccuracy(classifications, y_val)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Fold {fold+1} Accuracy: {accuracy:.2%}.\")\n",
    "\n",
    "    print(f\"\\nAverage cross-validated accuracy is {np.mean(accuracies):.2%}\\n\")\n",
    "\n",
    "CrossValidate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
